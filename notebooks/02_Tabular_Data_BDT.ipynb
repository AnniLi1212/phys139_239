{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tabular Data and BDTs: Classifying Collider Events\n",
    "Authors: Javier Duarte, Raghav Kansal"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "files = {\n",
    "    \"bkg\": (os.path.join(\"data\", \"ntuple_4mu_bkg.root\"), \"https://zenodo.org/record/3901869/files/ntuple_4mu_bkg.root\"),\n",
    "    \"VV\": (os.path.join(\"data\", \"ntuple_4mu_VV.root\"), \"https://zenodo.org/record/3901869/files/ntuple_4mu_VV.root\"),\n",
    "}\n",
    "\n",
    "for name, url in files.values():\n",
    "    if not os.path.exists(name):\n",
    "        print(f\"Downloading file {name}.\")\n",
    "\n",
    "        os.makedirs(os.path.dirname(name), exist_ok=True)\n",
    "\n",
    "        response = requests.get(url)\n",
    "        with open(name, \"wb\") as file:\n",
    "            file.write(response.content)\n",
    "    print(f\"File {name} is downloaded\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading `NumPy` arrays\n",
    "Now we load two different `NumPy` arrays. \n",
    "One corresponding to the VV signal and one corresponding to the background."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import uproot\n",
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "# fix random seed for reproducibility\n",
    "seed = 42\n",
    "np.random.seed(seed)\n",
    "\n",
    "treename = \"HZZ4LeptonsAnalysisReduced\"\n",
    "\n",
    "VARS = [\"f_mass4l\", \"f_massjj\"]\n",
    "cut = \"(f_mass4l > -999) & (f_massjj > -999)\"\n",
    "\n",
    "Xs = []\n",
    "Ys = []\n",
    "for key in files.keys():\n",
    "    with uproot.open(f\"{files[key][0]}:{treename}\") as tree:\n",
    "        arrays = tree.arrays(VARS, cut=cut, library=\"np\")\n",
    "        Xs.append(np.stack([arrays[var] for var in VARS], axis=-1))\n",
    "        Ys.append(np.full_like(arrays[VARS[0]], key == \"VV\", dtype=float))\n",
    "\n",
    "X = np.concatenate(Xs)\n",
    "Y = np.concatenate(Ys)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing the data\n",
    "\n",
    "We will split the data into two parts (one for training+validation and one for testing). \n",
    "We will also apply \"standard scaling\" preprocessing: http://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html i.e. making the mean = 0 and the RMS = 1 for all input variables (based **only** on the training/validation dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train_val, X_test, Y_train_val, Y_test = train_test_split(X, Y, test_size=0.2, random_state=7)\n",
    "\n",
    "# preprocessing: standard scalar\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler().fit(X_train_val)\n",
    "X_train_val = scaler.transform(X_train_val)\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define the model\n",
    "We'll start with a dense (fully-connected) NN layer.\n",
    "Our model will have a single fully-connected hidden layer with the same number of neurons as input variables. \n",
    "The weights are initialized using a small Gaussian random number. \n",
    "We will switch between linear and tanh activation functions for the hidden layer.\n",
    "The output layer contains a single neuron in order to make predictions. \n",
    "It uses the sigmoid activation function in order to produce a probability output in the range of 0 to 1.\n",
    "\n",
    "We are using the `binary_crossentropy` loss function during training, a standard loss function for binary classification problems. \n",
    "We will optimize the model with the Adam algorithm for stochastic gradient descent and we will collect accuracy metrics while the model is trained.\n",
    "We will also define our early stopping criteria to prevent over-fitting and we will save the model based on the best `val_loss`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# baseline keras model\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.layers import Input, Activation, Dense, Convolution2D, MaxPooling2D, Dropout, Flatten\n",
    "\n",
    "inputs = Input(shape=(len(VARS),), name=\"input\")\n",
    "outputs = Dense(1, name=\"output\", kernel_initializer=\"normal\", activation=\"sigmoid\")(inputs)\n",
    "\n",
    "# creae the model\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "# compile the model\n",
    "model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=[\"accuracy\"])\n",
    "# print the model summary\n",
    "model.summary()\n",
    "\n",
    "# early stopping callback\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stopping = EarlyStopping(monitor=\"val_loss\", patience=10)\n",
    "\n",
    "# model checkpoint callback\n",
    "# this saves our model architecture + parameters into dense_model.h5\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "\n",
    "model_checkpoint = ModelCheckpoint(\n",
    "    \"dense_model.h5\", monitor=\"val_loss\", verbose=0, save_best_only=True, save_weights_only=False, mode=\"auto\", save_freq=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run training \n",
    "Here, we run the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train classifier\n",
    "history = model.fit(\n",
    "    X_train_val,\n",
    "    Y_train_val,\n",
    "    epochs=100,\n",
    "    batch_size=1024,\n",
    "    verbose=0,  # switch to 1 for more verbosity\n",
    "    callbacks=[early_stopping, model_checkpoint],\n",
    "    validation_split=0.25,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot performance\n",
    "Here, we plot the history of the training and the performance in a ROC curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "%matplotlib inline\n",
    "# plot loss vs epoch\n",
    "plt.figure(figsize=(15, 10))\n",
    "ax = plt.subplot(2, 2, 1)\n",
    "ax.plot(history.history[\"loss\"], label=\"loss\")\n",
    "ax.plot(history.history[\"val_loss\"], label=\"val_loss\")\n",
    "ax.legend(loc=\"upper right\")\n",
    "ax.set_xlabel(\"epoch\")\n",
    "ax.set_ylabel(\"loss\")\n",
    "\n",
    "# plot accuracy vs epoch\n",
    "ax = plt.subplot(2, 2, 2)\n",
    "ax.plot(history.history[\"accuracy\"], label=\"acc\")\n",
    "ax.plot(history.history[\"val_accuracy\"], label=\"val_acc\")\n",
    "ax.legend(loc=\"upper left\")\n",
    "ax.set_xlabel(\"epoch\")\n",
    "ax.set_ylabel(\"acc\")\n",
    "\n",
    "# Plot ROC\n",
    "Y_predict = model.predict(X_test)\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(Y_test, Y_predict)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "ax = plt.subplot(2, 2, 3)\n",
    "ax.plot(fpr, tpr, lw=2, color=\"cyan\", label=\"auc = %.3f\" % (roc_auc))\n",
    "ax.plot([0, 1], [0, 1], linestyle=\"--\", lw=2, color=\"k\", label=\"random chance\")\n",
    "ax.set_xlim([0, 1.0])\n",
    "ax.set_ylim([0, 1.0])\n",
    "ax.set_xlabel(\"false positive rate\")\n",
    "ax.set_ylabel(\"true positive rate\")\n",
    "ax.set_title(\"receiver operating curve\")\n",
    "ax.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot NN output vs input variables\n",
    "Here, we will plot the NN output and devision boundary as a function of the input variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a regular 2D grid for the inputs\n",
    "myXI, myYI = np.meshgrid(np.linspace(-2, 2, 200), np.linspace(-2, 2, 200))\n",
    "# print shape\n",
    "print(myXI.shape)\n",
    "\n",
    "# run prediction at each point\n",
    "myZI = model.predict(np.c_[myXI.ravel(), myYI.ravel()])\n",
    "myZI = myZI.reshape(myXI.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below shoes how to plot the NN output and decision boundary. Does it look optimal?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "plt.figure(figsize=(20, 7))\n",
    "\n",
    "# plot contour map of NN output\n",
    "# overlaid with test data points\n",
    "ax = plt.subplot(1, 2, 1)\n",
    "cm = plt.cm.RdBu\n",
    "cm_bright = ListedColormap([\"#FF0000\", \"#0000FF\"])\n",
    "cont_plot = ax.contourf(myXI, myYI, myZI, cmap=cm, alpha=0.8)\n",
    "ax.scatter(X_test[:, 0], X_test[:, 1], c=Y_test, cmap=cm_bright, edgecolors=\"k\")\n",
    "ax.set_xlim(-2, 2)\n",
    "ax.set_ylim(-2, 2)\n",
    "ax.set_xlabel(VARS[0])\n",
    "ax.set_ylabel(VARS[1])\n",
    "plt.colorbar(cont_plot, ax=ax, boundaries=[0, 1], label=\"NN output\")\n",
    "\n",
    "# plot decision boundary\n",
    "# overlaid with test data points\n",
    "ax = plt.subplot(1, 2, 2)\n",
    "cm = plt.cm.RdBu\n",
    "cm_bright = ListedColormap([\"#FF0000\", \"#0000FF\"])\n",
    "cont_plot = ax.contourf(myXI, myYI, myZI > 0.5, cmap=cm, alpha=0.8)\n",
    "ax.scatter(X_test[:, 0], X_test[:, 1], c=Y_test, cmap=cm_bright, edgecolors=\"k\")\n",
    "ax.set_xlim(-2, 2)\n",
    "ax.set_ylim(-2, 2)\n",
    "ax.set_xlabel(VARS[0])\n",
    "ax.set_ylabel(VARS[1])\n",
    "plt.colorbar(cont_plot, ax=ax, boundaries=[0, 1], label=\"NN output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question 1:** What happens if you increase/decrease the number of hidden layers?\n",
    "\n",
    "**Question 2:** What happens if you increase/decrease the number of nodes per hidden layer?\n",
    "\n",
    "**Question 3:** What happens if you add/remove dropout?\n",
    "\n",
    "**Question 4:** What happens if you add/remove early stopping?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "phys139",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15 | packaged by conda-forge | (main, Nov 22 2022, 08:55:37) \n[Clang 14.0.6 ]"
  },
  "vscode": {
   "interpreter": {
    "hash": "d0ea348b636367bcdf67fd2d6d24251712b38670f61fdee14f28eb58fe74f081"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
